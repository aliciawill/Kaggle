{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Moa prediction\n\n- main strategy\n    - overfit to private"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## prediction flow\n\n- make 5 models\n    - simple NN with autoencoder\n    - resnet\n    - 2 headed NN??\n    - RNN\n    - TabNet\n    \n- staking all of them\n- ensemble 6 models above"},{"metadata":{},"cell_type":"markdown","source":"## prediction consider\n\n- cv와 lb간의 성능 상관관계가 없었기 때문에 lb를 통해서 성능 검증\n- 파라미터 튜닝은 oof 성능을 통해서 검증\n- label smoothing??"},{"metadata":{},"cell_type":"markdown","source":"# options"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed, batch, epoch config\n\nLABEL_SMOOTHING=0.001\n\nSEEDS = [120,2524]\nSPLITS = 7\nBATCH_SIZE = 64\nEPOCHS = 60\n\nVARIANCE_THRESHOLD = True\n\nADD_NON_TRAIN = True\nADD_SEEDS = SEEDS\nADD_SPLITS = SPLITS\nADD_BATCH_SIZE = BATCH_SIZE\n\nADD_EPOCHS = EPOCHS\n\nRUN_SNN = True\nSNN_SEEDS = SEEDS + [42]\nSNN_SPLITS = SPLITS\nSNN_BATCH_SIZE = BATCH_SIZE\nSNN_EPOCHS = EPOCHS\n\nRUN_NN = True\nNN_SEEDS = SEEDS \nNN_SPLITS = SPLITS\nNN_BATCH_SIZE = BATCH_SIZE\nNN_EPOCHS = EPOCHS\n\nRUN_NN2 = True\nNN2_SEEDS = SEEDS\nNN2_SPLITS = SPLITS\nNN2_BATCH_SIZE = BATCH_SIZE\nNN2_EPOCHS = EPOCHS\n\nRUN_RNN = True\nRNN_SEEDS = SEEDS\nRNN_SPLITS = SPLITS\nRNN_BATCH_SIZE = 128\nRNN_EPOCHS = EPOCHS\n\nRUN_TABNET = True\nTAB_SEEDS = SEEDS\nTAB_SPLITS = SPLITS\nTAB_BATCH_SIZE = BATCH_SIZE\nTAB_EPOCHS = EPOCHS\n\nRUN_STACKING = True\nSTK_SEEDS = SEEDS + [42]\nSTK_SPLITS = SPLITS \nSTK_EPOCHS = EPOCHS\nSTK_BATCH_SIZE = BATCH_SIZE\n\nRE_RUN_NN = True\nRE_NN_SEEDS = SEEDS","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nimport sys\n# sys.path.append('../input/pytorchtabnet/tabnet-develop')\n# from pytorch_tabnet import tab_network\n# sys.path.append('../input/interactivestratification/iterative-stratification-master')\n# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":3,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/iterative-stratification/iterative-stratification-master\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\nBuilding wheels for collected packages: iterative-stratification\n  Building wheel for iterative-stratification (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=8454e446ea0d9fa572dd962b6708afcb15309a91e07594ce5cb5eeb13c46c821\n  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\nSuccessfully built iterative-stratification\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.6\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall -y typing # this should avoid  AttributeError: type object 'Callable' has no attribute '_abc_registry'\n\n!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade\n\nfrom pytorch_tabnet import tab_network","execution_count":4,"outputs":[{"output_type":"stream","text":"Found existing installation: typing 3.7.4.3\nUninstalling typing-3.7.4.3:\n  Successfully uninstalled typing-3.7.4.3\nCollecting pytorch_tabnet\n  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to /tmp/pip-install-e_4af28s/pytorch-tabnet\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied, skipping upgrade: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.6.0)\nRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.18.5)\nRequirement already satisfied, skipping upgrade: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.4.1)\nRequirement already satisfied, skipping upgrade: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (0.23.2)\nRequirement already satisfied, skipping upgrade: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (4.45.0)\nRequirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (0.18.2)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch_tabnet) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch_tabnet) (0.14.1)\nBuilding wheels for collected packages: pytorch-tabnet\n  Building wheel for pytorch-tabnet (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-tabnet: filename=pytorch_tabnet-2.0.1-py3-none-any.whl size=30355 sha256=4e7b41a5b0fe28d715328668819ce8e5664ddf40fa105ab2640e3aecf69b082a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-th7b3x25/wheels/a6/8e/aa/6f5ef6a2e389c8b5f7ea1c74bbb03ece8773b03c2b8955c334\nSuccessfully built pytorch-tabnet\nInstalling collected packages: pytorch-tabnet\nSuccessfully installed pytorch-tabnet-2.0.1\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport torch\n\n# 시드값 고정\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)  \n    session_conf = tf.compat.v1.ConfigProto(\n        intra_op_parallelism_threads=1,\n        inter_op_parallelism_threads=1\n    )\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n    tf.compat.v1.keras.backend.set_session(sess)\n\n# 아래의 함수들이 어디서 사용되는지 확인\n# Loss 산정하는 함수인듯\ndef swish(x):\n    return x * K.sigmoid(x)\n\ndef show_metrics(valid_preds, show_each=True):\n    metrics = []\n    for _target in valid_preds.columns:\n      logloss = log_loss(train_targets_scored.iloc[:,1:].loc[:, _target], valid_preds.loc[:, _target])\n      metrics.append(logloss)\n      if show_each:\n        print(f'column: {_target}, log loss: {logloss}')\n    print(f'OOF Metric: {np.mean(metrics)}')\n\n    return metrics","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = pd.read_csv(f'../input/lish-moa/train_features.csv')\ntrain_targets_scored = pd.read_csv(f'../input/lish-moa/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(f'../input/lish-moa/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv(f'../input/lish-moa/test_features.csv')\nsample_submission = pd.read_csv(f'../input/lish-moa/sample_submission.csv')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_col = list(\n    set(train_targets_scored.columns.tolist())\\\n    - set(['sig_id'])\n)\n\ntemp_df = pd.DataFrame()\ntemp_df['col'] = ''\ntemp_df['value_0'] = 0\ntemp_df['ratio'] = 0.0\n\nfor col in test_col:\n    value0 = dict(\n        train_targets_scored[col].value_counts()\n    )[0]\n    \n    temp_df = temp_df.append(\n            {'col': col,\n             'value_0': value0,\n             'ratio': value0/len(train_targets_scored),\n            },\n            ignore_index = True\n    )","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# very imbalanced data\ntemp_df","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"                                           col  value_0     ratio\n0                         atm_kinase_inhibitor    23808  0.999748\n1    bacterial_50s_ribosomal_subunit_inhibitor    23734  0.996641\n2                   glutamate_receptor_agonist    23740  0.996893\n3                    sigma_receptor_antagonist    23778  0.998488\n4                              mucolytic_agent    23766  0.997984\n..                                         ...      ...       ...\n201           cc_chemokine_receptor_antagonist    23712  0.995717\n202               carbonic_anhydrase_inhibitor    23778  0.998488\n203      ubiquitin_specific_protease_inhibitor    23808  0.999748\n204                   rna_polymerase_inhibitor    23789  0.998950\n205             bacterial_dna_gyrase_inhibitor    23725  0.996263\n\n[206 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col</th>\n      <th>value_0</th>\n      <th>ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>atm_kinase_inhibitor</td>\n      <td>23808</td>\n      <td>0.999748</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bacterial_50s_ribosomal_subunit_inhibitor</td>\n      <td>23734</td>\n      <td>0.996641</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>glutamate_receptor_agonist</td>\n      <td>23740</td>\n      <td>0.996893</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sigma_receptor_antagonist</td>\n      <td>23778</td>\n      <td>0.998488</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mucolytic_agent</td>\n      <td>23766</td>\n      <td>0.997984</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>cc_chemokine_receptor_antagonist</td>\n      <td>23712</td>\n      <td>0.995717</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>carbonic_anhydrase_inhibitor</td>\n      <td>23778</td>\n      <td>0.998488</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>ubiquitin_specific_protease_inhibitor</td>\n      <td>23808</td>\n      <td>0.999748</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>rna_polymerase_inhibitor</td>\n      <td>23789</td>\n      <td>0.998950</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>bacterial_dna_gyrase_inhibitor</td>\n      <td>23725</td>\n      <td>0.996263</td>\n    </tr>\n  </tbody>\n</table>\n<p>206 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 유전, 세포와 관련된 피처들만 따로 변수저장\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'GENES: {len(GENES)}')\nprint(f'CELLS: {len(CELLS)}')","execution_count":10,"outputs":[{"output_type":"stream","text":"GENES: 772\nCELLS: 100\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\n- 분산 임계치(variance Threshold)\n    - test셋을 사용한 것(leaky), 사용하지 않은 것의 분산임계값\n    - 화자는 오버핏을 예상하고 테스트셋을 포함(leaky)\n    - RNN을 제외한 모든 학습테스트에 적용\n    - RNN에서는 non-leaky 분산 임계치 사용\n    - 분산임계치란?\n<br></br>\n<br></br>\n- PCA features\n    - leaky 사용\n    - 주성분이 많은 피처, 적은 피처 두가지를 사용  \n<br></br>\n<br></br>\n- Statistical features\n    - 'sum', 'mean', 'std', 'skew', 'kurt', 'median'\n <br></br>\n<br></br>   \n- Rank Gauss, Standard Scaler\n    - input 피처의 분포를 아웃풋 때 정규화를 통해서 가우시안 분포를 따르도록 하는 것\n  <br></br>\n<br></br>    \n- Non scored targets\n    - ??"},{"metadata":{"trusted":true},"cell_type":"code","source":"_X_train = train_features.copy()\n_X_test = test_features.copy()\n_y_train = train_targets_scored.copy()\n","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## variance threshold\n- 메인 아이디어\n    - 분산이 작은 변수는 정보의 함유량이 낮다.\n    - 각 변수에 대한 분산을 계산한 뒤 임계값을 기준으로 임계값보다 낮은 분산을 가진 변수를 제거한다\n    - Make sure features have the same scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 분산 임계치 라이브러리\nfrom sklearn.feature_selection import VarianceThreshold","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if VARIANCE_THRESHOLD:\n    # 유전, 세포 관련 컬럼들만 모두 추출\n    vt_cols = _X_train.loc[:,GENES+CELLS].columns\n\n    # 분산 임계치를 0.8로 설정\n    # 임계점을 기준으로 어떻게 수치변환을 하는지?\n    # 수치변환을 하는 것이 아니라 각 변수별 분산을 구한다음 분석가가 정한 임계점을 기준으로(threshold)\n    # 그 값보다 분산이 적은 변수들을 제거하는 방식으로 변수를 선택한다\n    vh = VarianceThreshold(0.8)\n    train_trans = pd.DataFrame(vh.fit_transform(_X_train.loc[:,GENES+CELLS]))\n    # 분산임계치의 get_support() : Get a mask, or integer index, of the features selected\n    # 변수 선택: 분산임계치를 기준으로 변수선택\n    # .get_support() 함수가 분산이 기준치 미달인 변수들을 selection해주는 함수\n    vt_cols = vt_cols[vh.get_support()]\n\n    # leaky - test셋까지 포함해서 변환\n    vt_cols_leaky = _X_train.loc[:,GENES+CELLS].columns\n\n    vh_leaky = VarianceThreshold(0.8)\n    # leaky한 방법이 테스트셋을 행병합을 통해서 병합한 뒤 진행하는거군...\n    # leaky, non-leaky의 train 데이터 변수가 다르게 나옴\n    train_trans = pd.DataFrame(vh_leaky.fit_transform(pd.concat([_X_train.loc[:,GENES+CELLS], _X_test.loc[:,GENES+CELLS]])))\n    vt_cols_leaky = vt_cols_leaky[vh_leaky.get_support()]\n","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## categorical feature wrangling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical feature pre-processing\n\n# cp_type - 범주형 변수 cp 제거, trt_cp에 대한 예측을 해야함\ndef deal_with_cp_type(_X_train, _X_test):\n    train_trt_index = _X_train.cp_type == 'trt_cp'\n    test_trt_index = _X_test.cp_type == 'trt_cp'\n\n    _X_train = _X_train[train_trt_index]\n    _X_test = _X_test\n    del _X_train['cp_type']\n    del _X_test['cp_type']\n\n    return _X_train, _X_test, train_trt_index, test_trt_index\n\n_X_train, _X_test, train_trt_index, test_trt_index = deal_with_cp_type(_X_train, _X_test)\n\n# cp_dose 더미 변수화\ndef deal_with_cp_dose(_X_train, _X_test):\n    _X_train = pd.concat([_X_train, pd.get_dummies(_X_train.cp_dose)], axis=1)\n    del _X_train['cp_dose']\n    _X_test = pd.concat([_X_test, pd.get_dummies(_X_test.cp_dose)], axis=1)\n    del _X_test['cp_dose']\n\n    return _X_train, _X_test\n\n_X_train, _X_test = deal_with_cp_dose(_X_train, _X_test)\n\n\n# sig_id 피처 제거\ndef deal_with_sig_id(_X_train, _X_test):\n    del _X_train['sig_id']\n    del _X_test['sig_id']\n\n    return _X_train, _X_test\n\n_X_train, _X_test = deal_with_sig_id(_X_train, _X_test)\n\n\n# 예측해야하는 cp_type index만 추출\ndef deal_with_y(_y_train, train_trt_index):\n    _y_train = _y_train[train_trt_index]\n    del _y_train['sig_id']\n\n    return _y_train\n\n_y_train = deal_with_y(_y_train, train_trt_index)\n\n# I guess 'D1' should be eliminated but anyway, I was using it. \n\nBASE_COLS = ['cp_time', 'D1', 'D2']\n\n_X_train_dae = _X_train.copy()\n_X_test_dae = _X_test.copy()\n\n_X_train = _X_train.reset_index()","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# target non-score\n- 해당 대회에서 제공해준 데이터(사용할지 여부는 선택사항)\n- Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n- 해당 데이터를 어떻게 활용할지는 밑에서 설명"},{"metadata":{"trusted":true},"cell_type":"code","source":"non_scored_ones = pd.DataFrame()\nnon_scored_ones['col_name'] = ''\nnon_scored_ones['item_counts'] = 0\n\nfor col in train_targets_nonscored.columns[1:]:\n\n    item_counts = len(train_targets_nonscored[train_targets_nonscored[col] == 1])\n\n    non_scored_ones = non_scored_ones.append({'col_name':col, 'item_counts':item_counts}, ignore_index=True)\n\nnon_scored_target_cols = non_scored_ones[non_scored_ones.item_counts > 10].col_name\n\ny_non_train = train_targets_nonscored[non_scored_target_cols]\ny_non_train = y_non_train[train_trt_index].reset_index(drop=True)","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 주성분 수는 50개로 설정\ndef yield_pca(_X_train, _X_test, prefix, decomp_cols, comp=50, random_state=42):\n    # pca confg 할당\n    pca = PCA(n_components=comp, random_state=random_state)\n    print(pca.n_components)\n    # 주성분 분석을 진행하기 위해서 train, test데이터 병합 후 주성분 분석\n    # pca fitting\n    pca.fit(pd.concat([_X_train.loc[:,decomp_cols], _X_test.loc[:,decomp_cols]]))\n    # 유전과 관련된 변수 약 700개를 차원축소 진행: 주성분 갯수를 50개로 했을 때 누적 설명력이 66%밖에 안됨\n    # 적어도 75% 이상이 될 수 있도록 주성분 갯수를 조정했어야하지 않을까? 주성분 100개일 때 72%\n    _X_train_PCA = pca.transform(_X_train[decomp_cols])\n    _X_test_PCA = pca.transform(_X_test[decomp_cols])\n\n    _X_train_PCA = pd.DataFrame(_X_train_PCA, columns=[f'pca_{prefix}-{i}' for i in range(comp)])\n    _X_test_PCA = pd.DataFrame(_X_test_PCA, columns=[f'pca_{prefix}-{i}' for i in range(comp)])\n\n    return _X_train_PCA, _X_test_PCA","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 주성분을 여러개로 세팅한 뒤 만든 데이터셋에 대해서 리스트업\n_X_train_G_PCA, _X_test_G_PCA = yield_pca(_X_train, _X_test, 'G', GENES, 90)\n_X_train_C_PCA, _X_test_C_PCA = yield_pca(_X_train, _X_test, 'C', CELLS, 27)\n# Dense를 만든 이유가 뭐지... -> 질문 리스트\n_X_train_G_PCA_Dense, _X_test_G_PCA_Dense = yield_pca(_X_train, _X_test, 'Gd', GENES, 8)\n_X_train_C_PCA_Dense, _X_test_C_PCA_Dense = yield_pca(_X_train, _X_test, 'Cd', CELLS, 1)","execution_count":18,"outputs":[{"output_type":"stream","text":"90\n27\n8\n1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"_X_train = pd.concat([_X_train, _X_train_G_PCA], axis=1)\n_X_train = pd.concat([_X_train, _X_train_C_PCA], axis=1)\n_X_train = pd.concat([_X_train, _X_train_G_PCA_Dense], axis=1)\n_X_train = pd.concat([_X_train, _X_train_C_PCA_Dense], axis=1)\n\n_X_test = pd.concat([_X_test, _X_test_G_PCA], axis=1)\n_X_test = pd.concat([_X_test, _X_test_C_PCA], axis=1)\n_X_test = pd.concat([_X_test, _X_test_G_PCA_Dense], axis=1)\n_X_test = pd.concat([_X_test, _X_test_C_PCA_Dense], axis=1)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 컬럼만 따로 변수화\nPCA_G = [col for col in _X_train.columns if col.startswith('pca_G-')] \\\n+ [col for col in _X_train.columns if col.startswith('pca_Gd-')]\nPCA_C = [col for col in _X_train.columns if col.startswith('pca_C-')] \\\n+ [col for col in _X_train.columns if col.startswith('pca_Cd-')]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# statistic feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_feature(data, target_feats, create, prefix):\n    target_data = data.loc[:,target_feats]\n    # getattr 함수: 모든 변수에 대해서 통계값을 뽑고 싶을 때 getattr(data, method)를 입력해주면 됨\n    invoker = getattr(target_data, create)\n    return pd.DataFrame(invoker(axis=1), columns=[f'{prefix}_{create}'])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for method in ['sum', 'mean', 'std', 'skew', 'kurt', 'median']: # min max\n    _X_train = pd.concat([_X_train, make_feature(_X_train, GENES+CELLS, method, 'gce')], axis = 1)\n    _X_train = pd.concat([_X_train, make_feature(_X_train, GENES, method, 'ge')], axis = 1)\n\n    _X_test = pd.concat([_X_test, make_feature(_X_test, GENES+CELLS, method, 'gce')], axis = 1)\n    _X_test = pd.concat([_X_test, make_feature(_X_test, GENES, method, 'ge')], axis = 1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stat변수들 변수화\nGC_EX = [col for col in _X_train.columns if col.startswith('gce_')]\nG_EX = [col for col in _X_train.columns if col.startswith('ge_')] ","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 변수 스케일 및 사분위수 변환"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_col_names = _X_train.columns\ntest_col_names = _X_test.columns\n\n# 사분위수 변환할 변수들 리스트업\nTRANSFORM_TARGET_COLS = BASE_COLS + GENES + CELLS + PCA_G + PCA_C + G_EX + GC_EX","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rank_guass 변환 방식: quantileTransformer - 분포를 정규분포곡선 형식으로 변환\n# 지정된 분위수에 맞게 데이터를 변환하는 함수(n_quantiles: default: 1000)\n\n# 스케일을하거나 변환 및 주성분 분석을 진행할 때는 train, test셋 모두 병합을 한 뒤 진행을 한다.\n\n# 모든 변수들에 대해서 rank_gauss방법을 사용한 이유는??\ndef rank_gauss(_X_train, _X_test, cols=TRANSFORM_TARGET_COLS, random_state=42):\n    qt = QuantileTransformer(random_state=random_state, output_distribution='normal')\n    qt.fit(pd.concat([_X_train[cols], _X_test[cols]]))\n    _X_train[cols] = qt.transform(_X_train[cols])\n    _X_test[cols] = qt.transform(_X_test[cols])\n\n    return _X_train, _X_test\n\n# 표준정규분포 스케일러\ndef standard_scaler(_X_train, _X_test, cols=TRANSFORM_TARGET_COLS):\n    ss = StandardScaler()\n    ss.fit(pd.concat([_X_train[cols], _X_test[cols]]))\n    _X_train[cols] = ss.transform(_X_train[cols])\n    _X_test[cols] = ss.transform(_X_test[cols])\n\n    return _X_train, _X_test\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_X_train, _X_test = rank_gauss(_X_train, _X_test)\n_X_train, _X_test = standard_scaler(_X_train, _X_test)\n\n_X_train.columns = train_col_names\n_X_test.columns = test_col_names","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 지금까지 만든 변수들을 이용해서 다시 분산임계치 변수선택"},{"metadata":{"trusted":true},"cell_type":"code","source":"if VARIANCE_THRESHOLD:\n    # 기존 분산임계치로 추출한 변수 + pca, stat으로 생성한 변수들을 이용하여 변수선택\n    vt_cols = vt_cols.values.tolist() + BASE_COLS + PCA_G + PCA_C + GC_EX + G_EX\n\n    _X_train_nl = pd.concat([_X_train.iloc[:,:1], _X_train.loc[:,vt_cols]], axis=1)\n    _X_test_nl = _X_test.loc[:,vt_cols]\n    \n        \n    vt_cols_leaky = vt_cols_leaky.values.tolist() + BASE_COLS + PCA_G + PCA_C + GC_EX + G_EX\n\n    _X_train = pd.concat([_X_train.iloc[:,:1], _X_train.loc[:,vt_cols_leaky]], axis=1)\n    _X_test = _X_test.loc[:,vt_cols_leaky]\n\n    GENES = [col for col in _X_train.columns if col.startswith('g-')]\n    CELLS = [col for col in _X_train.columns if col.startswith('c-')]","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set_index: 특정 컬럼을 인덱스로 옮기는 것\n_X_train = _X_train.set_index('index')\nX_train = _X_train.copy()\nX_test = _X_test.copy()\n\n# no leakage\n_X_train_nl = _X_train_nl.set_index('index')\nX_train_nl = _X_train_nl.copy()\nX_test_nl = _X_test_nl.copy()\n\ny_train = _y_train.copy()","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 모델링\n- Non-scored target model\n- simple NN\n- RNN\n- Resnet\n- TabNet"},{"metadata":{},"cell_type":"markdown","source":"## Non-scored target model\n- non_target 관련 데이터에 대한 모델링이네?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_predict(\n    name,\n    model_func, \n    _X_i_train, \n    _y_i_train, \n    _X_i_test, \n    orig_targets,\n    result_template,\n    seeds, \n    splits, \n    epochs, \n    batch_size, \n    shuffle_rows=False, \n    pick_col_size=800, \n    do_show_metrics=True, \n    show_each_metrics=True\n):\n    \n    st = time.time()\n\n    # 리스트인지 확인하는 함수\n    is_list = isinstance(_X_i_train, list)\n\n    # train_targets_nonscored 데이터에서 필요한 데이터들만 추출한 것\n    val_result = orig_targets.copy()\n    val_result.loc[:, :] = 0\n\n    # \n    sub_result = result_template.copy()\n    sub_result.loc[:, 1:] = 0\n    \n    \n    # 시드를 여러개 리스트업 해놓고 과적합 막기 위한 장치로 세팅??\n    for h, seed in enumerate(seeds):\n\n        seed_everything(seed)\n        \n        # 예측해야하는 output자체가 multilable classification이므로 \n        # multilabelstratifiedkfold를 사용\n        for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=splits, random_state=seed, shuffle=True)\n                                        .split(_y_i_train, _y_i_train)):\n            print(f'Fold {i+1}')\n            # array 형태로 뎅터 형식을 맞춰야 함\n            if is_list:\n                _X_train = [_X_i_train[0].loc[:,:].values[train_idx], _X_i_train[1].loc[:,:].values[train_idx]]\n                _X_valid = [_X_i_train[0].loc[:,:].values[valid_idx], _X_i_train[1].loc[:,:].values[valid_idx]]\n            else:\n        \n                _X_train = _X_i_train.loc[:,:].values[train_idx]\n                _X_valid = _X_i_train.loc[:,:].values[valid_idx]\n            \n            # 예측 타겟 데이터도 배열 형태\n            _y_train = _y_i_train.values[train_idx]\n            _y_valid = _y_i_train.values[valid_idx]\n            \n            \n            # 모델에 넣을 인풋 사이즈(변수의 갯수를 inPuttkdlwmfh sjgdma)\n            if is_list:\n                model = model_func(len(_X_i_train[0].columns), len(_X_i_train[1].columns))\n            else:\n                model = model_func(len(_X_i_train.columns))\n            \n            # NN 모델 학습 -> 학습에 필요한 파라미터에 대해서 공부가 필요\n            model.fit(_X_train, _y_train,\n                    validation_data=(_X_valid, _y_valid),\n                    epochs=epochs, batch_size=batch_size,\n                    callbacks=[\n                        ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n                        , ModelCheckpoint(f'{name}_{seed}_{i}.hdf5', monitor = 'val_logloss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n                        , EarlyStopping(monitor = 'val_logloss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True)\n                    ], verbose=2)\n            \n            # 학습된 모델의 가중치를 적재\n            model.load_weights(f'{name}_{seed}_{i}.hdf5')\n            val_result.iloc[_y_i_train.iloc[valid_idx,:].index.values, :] += model.predict(_X_valid)\n\n            if is_list:\n                sub_result.loc[test_trt_index, sub_result.columns[1:]] += model.predict([_X_i_test[0].loc[test_trt_index, :], _X_i_test[1].loc[test_trt_index, :]])\n            else:\n                sub_result.loc[test_trt_index, sub_result.columns[1:]] += model.predict(_X_i_test.loc[test_trt_index, :])\n\n            print('')\n\n        tmp_result = val_result.copy()\n        tmp_result.iloc[:,1:] = val_result.iloc[:,1:] / (h + 1)\n        print(f' ---- seed:{seed}, ensemble:{h + 1}')\n        if do_show_metrics:\n            _ = show_metrics(tmp_result, show_each=False)\n\n    val_result.iloc[:,1:] = val_result.iloc[:,1:] / len(seeds)\n    if do_show_metrics:\n        metrics = show_metrics(val_result, show_each=show_each_metrics)\n    else:\n        metrics = None\n\n    sub_result.iloc[:, 1:] = sub_result.iloc[:, 1:] / (len(seeds) * splits)\n\n    print(f' elapsed: {time.time() - st}')\n\n    return sub_result, val_result, metrics","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non_scored_target_cols: non_scored 데이터에서 가져온 컬럼(선택사항 컬럼) - validation셋인가?\n# train_trt_index: 특정 범주형 데이터의 변수를 제외한 예측하고자하는 인덱스들\ntrain_and_predict(\n    'ADD', \n    create_add_model, \n    X_train, \n    y_non_train,\n    X_test,\n    train_targets_nonscored[non_scored_target_cols].loc[train_trt_index,:],\n    non_scored_template,\n    ADD_SEEDS, \n    ADD_SPLITS, \n    ADD_EPOCHS, \n    ADD_BATCH_SIZE, \n    shuffle_rows=False, \n    do_show_metrics=False,\n    show_each_metrics=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Make Extra Features from non-scored targets\n- To use non scored targets, I made a model and created non scored targets for test set.\n- option 요소로 주어진 데이터를 활용하기 위해서 non-scored targets 모델을 생성함"},{"metadata":{"trusted":true},"cell_type":"code","source":"p_min = 0.001\np_max = 0.999\n\n# logloss 값을 학습시마다 지속적으로 반환\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 어떤 역할을 하는 함수? bias_initializer\noutput_bias=tf.keras.initializers.Constant(-np.log(y_train.mean(axis=0).to_numpy()))","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_add_model(input_dim):\n    print(f'the input dim is {input_dim}')\n\n    # M: keras 모델 - sequential 모델을 초기값으로 할당\n    model = M.Sequential()\n    # 인풋레이어 사이즈 정의\n    model.add(L.Input(input_dim))\n    # 레이어 Batchnormalization: 학습안정화 및 성능 개선\n    # 신경망 모델의 인풋 데이터를 평균0, 분산1인 표준정규분포 상태로 만들어서 네트워크 학습이 잘 진행되도록 돕는 방식\n    model.add(L.BatchNormalization())\n    # regularization 효과를 창출 / 레이어의 모든 노드에서 나가는 activation을 특정 확률로 지움\n    # 즉, 모델이 특정 환경에서 반복적으로 학습을 해서 도출하는 결과를 과하게 학습하지 않도록 만드는 것을 말함\n    model.add(L.Dropout(0.5))\n    # elu: relu\n    model.add(WeightNormalization(L.Dense(input_dim, activation='elu')))\n\n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.4))\n    model.add(WeightNormalization(L.Dense(512, activation='selu')))\n    \n    model.add(L.BatchNormalization())\n    model.add(L.Dropout(0.5))\n    # output layer, 가중치 정규화 방식 사용\n    model.add(WeightNormalization(L.Dense(y_non_train.shape[1], activation='sigmoid',\n                                          bias_initializer=tf.keras.initializers.Constant(-np.log(y_non_train.mean(axis=0).to_numpy())\n                                          ))))\n    \n    # model을 compile하는 이유는?\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=1e-3), sync_period=10),\n                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING), metrics=logloss)\n\n    return model","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non-scored target 데이터를 통한 예측 templates 생성\nnon_scored_template = sample_submission.copy()\nnon_scored_template = pd.DataFrame(non_scored_template.pop('sig_id'))\nfor col in non_scored_target_cols:\n    non_scored_template[col] = 0.","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (train_idx, valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=120, shuffle=True)\n                                        .split(y_non_train, y_non_train)):\n            print(f'Fold {i+1}')\n        \n        \n    _X_train = X_train.loc[:,:].values[train_idx]\n    _X_valid = X_train.loc[:,:].values[valid_idx]\n\n    # 예측 타겟 데이터도 배열 형태\n    _y_train = y_non_train.values[train_idx]\n    _y_valid = y_non_train.values[valid_idx]\n\n    model = create_add_model(len(X_train.columns))\n    \n    ","execution_count":224,"outputs":[{"output_type":"stream","text":"the input dim is 947\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model= create_add_model(len(X_train.columns))","execution_count":35,"outputs":[{"output_type":"stream","text":"the input dim is 947\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)*0.8","execution_count":233,"outputs":[{"output_type":"execute_result","execution_count":233,"data":{"text/plain":"17558.4"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = range(0,17558)\nvalid_idx = range(17558,len(X_train))","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NN 모델 학습 -> 학습에 필요한 파라미터에 대해서 공부가 필요\nname = 'ADD'\nseeds = 120\ni = 0\nmodel.fit(X_train.loc[:,:].values[train_idx], y_non_train.loc[:,:].values[train_idx],\n        validation_data=(X_train.loc[:,:].values[train_idx], y_non_train.loc[:,:].values[train_idx]),\n        epochs=60, batch_size=64,\n        callbacks=[\n            ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n            , ModelCheckpoint(f'{name}_{seeds}_{i}.hdf5', monitor = 'val_logloss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n            , EarlyStopping(monitor = 'val_logloss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True)\n        ], verbose=2)\n","execution_count":37,"outputs":[{"output_type":"stream","text":"Epoch 1/60\n275/275 - 10s - loss: 0.0216 - logloss: 0.0193 - val_loss: 0.0144 - val_logloss: 0.0120\nEpoch 2/60\n275/275 - 9s - loss: 0.0126 - logloss: 0.0097 - val_loss: 0.0107 - val_logloss: 0.0077\nEpoch 3/60\n275/275 - 9s - loss: 0.0110 - logloss: 0.0078 - val_loss: 0.0103 - val_logloss: 0.0070\nEpoch 4/60\n275/275 - 9s - loss: 0.0106 - logloss: 0.0074 - val_loss: 0.0099 - val_logloss: 0.0066\nEpoch 5/60\n275/275 - 9s - loss: 0.0104 - logloss: 0.0071 - val_loss: 0.0095 - val_logloss: 0.0063\nEpoch 6/60\n275/275 - 9s - loss: 0.0101 - logloss: 0.0069 - val_loss: 0.0092 - val_logloss: 0.0060\nEpoch 7/60\n275/275 - 9s - loss: 0.0099 - logloss: 0.0067 - val_loss: 0.0090 - val_logloss: 0.0057\nEpoch 8/60\n275/275 - 9s - loss: 0.0097 - logloss: 0.0065 - val_loss: 0.0087 - val_logloss: 0.0055\nEpoch 9/60\n275/275 - 9s - loss: 0.0096 - logloss: 0.0063 - val_loss: 0.0085 - val_logloss: 0.0053\nEpoch 10/60\n275/275 - 9s - loss: 0.0095 - logloss: 0.0062 - val_loss: 0.0083 - val_logloss: 0.0051\nEpoch 11/60\n275/275 - 9s - loss: 0.0093 - logloss: 0.0060 - val_loss: 0.0082 - val_logloss: 0.0049\nEpoch 12/60\n275/275 - 9s - loss: 0.0092 - logloss: 0.0059 - val_loss: 0.0080 - val_logloss: 0.0047\nEpoch 13/60\n275/275 - 9s - loss: 0.0091 - logloss: 0.0058 - val_loss: 0.0078 - val_logloss: 0.0045\nEpoch 14/60\n275/275 - 9s - loss: 0.0090 - logloss: 0.0057 - val_loss: 0.0077 - val_logloss: 0.0044\nEpoch 15/60\n275/275 - 9s - loss: 0.0090 - logloss: 0.0056 - val_loss: 0.0076 - val_logloss: 0.0043\nEpoch 16/60\n275/275 - 9s - loss: 0.0089 - logloss: 0.0055 - val_loss: 0.0074 - val_logloss: 0.0041\nEpoch 17/60\n275/275 - 9s - loss: 0.0088 - logloss: 0.0054 - val_loss: 0.0073 - val_logloss: 0.0040\nEpoch 18/60\n275/275 - 9s - loss: 0.0087 - logloss: 0.0054 - val_loss: 0.0072 - val_logloss: 0.0039\nEpoch 19/60\n275/275 - 9s - loss: 0.0087 - logloss: 0.0053 - val_loss: 0.0071 - val_logloss: 0.0038\nEpoch 20/60\n275/275 - 10s - loss: 0.0086 - logloss: 0.0052 - val_loss: 0.0070 - val_logloss: 0.0036\nEpoch 21/60\n275/275 - 9s - loss: 0.0085 - logloss: 0.0051 - val_loss: 0.0069 - val_logloss: 0.0035\nEpoch 22/60\n275/275 - 9s - loss: 0.0085 - logloss: 0.0051 - val_loss: 0.0068 - val_logloss: 0.0034\nEpoch 23/60\n275/275 - 9s - loss: 0.0084 - logloss: 0.0050 - val_loss: 0.0067 - val_logloss: 0.0033\nEpoch 24/60\n275/275 - 9s - loss: 0.0084 - logloss: 0.0049 - val_loss: 0.0066 - val_logloss: 0.0033\nEpoch 25/60\n275/275 - 9s - loss: 0.0083 - logloss: 0.0049 - val_loss: 0.0065 - val_logloss: 0.0032\nEpoch 26/60\n275/275 - 9s - loss: 0.0083 - logloss: 0.0048 - val_loss: 0.0064 - val_logloss: 0.0031\nEpoch 27/60\n275/275 - 10s - loss: 0.0082 - logloss: 0.0048 - val_loss: 0.0064 - val_logloss: 0.0030\nEpoch 28/60\n275/275 - 9s - loss: 0.0082 - logloss: 0.0047 - val_loss: 0.0063 - val_logloss: 0.0029\nEpoch 29/60\n275/275 - 9s - loss: 0.0081 - logloss: 0.0046 - val_loss: 0.0062 - val_logloss: 0.0028\nEpoch 30/60\n275/275 - 9s - loss: 0.0081 - logloss: 0.0046 - val_loss: 0.0062 - val_logloss: 0.0028\nEpoch 31/60\n275/275 - 9s - loss: 0.0080 - logloss: 0.0045 - val_loss: 0.0061 - val_logloss: 0.0027\nEpoch 32/60\n275/275 - 9s - loss: 0.0080 - logloss: 0.0045 - val_loss: 0.0060 - val_logloss: 0.0026\nEpoch 33/60\n275/275 - 9s - loss: 0.0079 - logloss: 0.0044 - val_loss: 0.0060 - val_logloss: 0.0026\nEpoch 34/60\n275/275 - 9s - loss: 0.0079 - logloss: 0.0044 - val_loss: 0.0059 - val_logloss: 0.0025\nEpoch 35/60\n275/275 - 9s - loss: 0.0079 - logloss: 0.0044 - val_loss: 0.0059 - val_logloss: 0.0025\nEpoch 36/60\n275/275 - 9s - loss: 0.0078 - logloss: 0.0043 - val_loss: 0.0058 - val_logloss: 0.0024\nEpoch 37/60\n275/275 - 9s - loss: 0.0077 - logloss: 0.0042 - val_loss: 0.0058 - val_logloss: 0.0023\nEpoch 38/60\n275/275 - 9s - loss: 0.0077 - logloss: 0.0042 - val_loss: 0.0057 - val_logloss: 0.0023\nEpoch 39/60\n275/275 - 9s - loss: 0.0077 - logloss: 0.0041 - val_loss: 0.0057 - val_logloss: 0.0023\nEpoch 40/60\n275/275 - 9s - loss: 0.0077 - logloss: 0.0041 - val_loss: 0.0056 - val_logloss: 0.0022\nEpoch 41/60\n275/275 - 9s - loss: 0.0076 - logloss: 0.0041 - val_loss: 0.0056 - val_logloss: 0.0021\nEpoch 42/60\n275/275 - 9s - loss: 0.0076 - logloss: 0.0040 - val_loss: 0.0055 - val_logloss: 0.0021\nEpoch 43/60\n275/275 - 9s - loss: 0.0075 - logloss: 0.0040 - val_loss: 0.0055 - val_logloss: 0.0020\nEpoch 44/60\n275/275 - 9s - loss: 0.0075 - logloss: 0.0039 - val_loss: 0.0054 - val_logloss: 0.0020\nEpoch 45/60\n275/275 - 9s - loss: 0.0075 - logloss: 0.0039 - val_loss: 0.0054 - val_logloss: 0.0020\nEpoch 46/60\n275/275 - 9s - loss: 0.0074 - logloss: 0.0038 - val_loss: 0.0053 - val_logloss: 0.0019\nEpoch 47/60\n275/275 - 10s - loss: 0.0073 - logloss: 0.0038 - val_loss: 0.0053 - val_logloss: 0.0019\nEpoch 48/60\n275/275 - 9s - loss: 0.0073 - logloss: 0.0037 - val_loss: 0.0053 - val_logloss: 0.0018\nEpoch 49/60\n275/275 - 9s - loss: 0.0073 - logloss: 0.0037 - val_loss: 0.0052 - val_logloss: 0.0018\nEpoch 50/60\n275/275 - 9s - loss: 0.0072 - logloss: 0.0036 - val_loss: 0.0052 - val_logloss: 0.0018\nEpoch 51/60\n275/275 - 9s - loss: 0.0072 - logloss: 0.0036 - val_loss: 0.0052 - val_logloss: 0.0017\nEpoch 52/60\n\nEpoch 00052: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n275/275 - 9s - loss: 0.0072 - logloss: 0.0036 - val_loss: 0.0051 - val_logloss: 0.0017\nEpoch 53/60\n275/275 - 9s - loss: 0.0069 - logloss: 0.0034 - val_loss: 0.0051 - val_logloss: 0.0017\nEpoch 54/60\n275/275 - 9s - loss: 0.0069 - logloss: 0.0033 - val_loss: 0.0051 - val_logloss: 0.0017\nEpoch 55/60\n275/275 - 9s - loss: 0.0069 - logloss: 0.0033 - val_loss: 0.0051 - val_logloss: 0.0016\nEpoch 56/60\n\nEpoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n275/275 - 9s - loss: 0.0069 - logloss: 0.0033 - val_loss: 0.0051 - val_logloss: 0.0016\nEpoch 57/60\n275/275 - 9s - loss: 0.0069 - logloss: 0.0033 - val_loss: 0.0051 - val_logloss: 0.0016\nEpoch 58/60\n275/275 - 9s - loss: 0.0069 - logloss: 0.0033 - val_loss: 0.0051 - val_logloss: 0.0016\n","name":"stdout"},{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fdd186a4bd0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 예측 테스\nmodel.predict_proba(X_test)[0]","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"194"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"            g-0       g-2       g-3       g-4       g-5       g-7       g-8  \\\nindex                                                                         \n0      1.123421 -0.433396 -0.964591 -0.282548 -1.016012 -0.045026  0.698549   \n1      0.118038  0.262404  0.096686  1.204425  0.695217  0.558623 -0.474493   \n2      0.774613  1.417783 -0.114154 -0.020037  1.495514  0.358201  0.044292   \n3     -0.744187 -0.456376  0.772458  2.448425 -0.858771  0.298735 -0.139522   \n4     -0.457396  0.962230  0.981547  1.454602 -0.869376 -0.230595 -0.981388   \n...         ...       ...       ...       ...       ...       ...       ...   \n23808  0.236639  0.207837 -0.350717 -0.361947  0.575774 -0.276129 -0.860194   \n23809  0.206721 -0.253901 -0.789493 -0.718753  0.923262  0.508876 -0.554977   \n23810 -1.940733 -0.603128  1.308103 -1.058194  0.856762 -0.736422  0.651861   \n23812  0.810708  0.421287  0.315457  1.073907 -0.021461  0.085268  0.555475   \n23813 -1.265053 -0.286199  1.096373 -0.551023 -2.097983  1.457567  2.179353   \n\n            g-9      g-10      g-11  ...   gce_std  gce_skew  gce_kurt  \\\nindex                                ...                                 \n0     -0.307208  1.545668  0.167332  ... -0.231509 -1.382206  0.332903   \n1      0.840109 -1.250344 -0.585397  ... -0.311570 -0.121494  0.771762   \n2      1.247845 -0.655505 -0.777878  ...  0.729941  0.040962  0.375723   \n3     -1.371561 -1.013221 -0.505054  ...  1.102178 -0.094962 -1.108978   \n4      0.850767 -0.346712 -0.711659  ...  0.717398 -2.232172  1.788667   \n...         ...       ...       ...  ...       ...       ...       ...   \n23808 -0.022794  0.705877 -0.706175  ... -3.097103 -0.010938 -1.357223   \n23809 -0.033930  0.286984  0.192944  ... -0.112217 -2.063024  1.858930   \n23810  0.648623 -0.303515  1.757062  ... -0.216652 -0.060783 -0.178504   \n23812 -1.184586  0.738031  0.152950  ...  1.135048 -1.711244 -0.674890   \n23813  1.263361  2.134326  1.267512  ...  1.273049 -1.709709 -0.643381   \n\n       gce_median    ge_sum   ge_mean    ge_std   ge_skew   ge_kurt  ge_median  \nindex                                                                           \n0        0.391092 -0.466548 -0.466548 -0.200071 -1.323474  0.279847  -0.599367  \n1        0.957949 -0.069741 -0.069741 -0.230676 -0.019631  0.671087   0.448958  \n2       -0.627145 -0.682173 -0.682173  0.757799 -0.153181  0.254737  -0.724672  \n3       -1.260776 -0.239230 -0.239230  1.017870  0.557489 -1.011274  -0.109471  \n4        0.710104 -0.933620 -0.933620  0.757162 -2.147718  1.589138  -0.000033  \n...           ...       ...       ...       ...       ...       ...        ...  \n23808    0.846992  1.399084  1.399084 -2.973969 -0.201149 -1.318300   1.022804  \n23809    0.629524  0.068775  0.068775 -0.050632 -2.035350  1.766637   0.374497  \n23810   -0.345776  0.067837  0.067837 -0.242659 -0.186023 -0.149545  -0.529686  \n23812    2.056726 -0.815510 -0.815510  1.176933 -1.586650 -0.874018   1.813982  \n23813   -1.476488 -1.289733 -1.289733  1.112746 -1.046423 -0.569578  -1.361960  \n\n[21948 rows x 947 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>g-0</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>g-7</th>\n      <th>g-8</th>\n      <th>g-9</th>\n      <th>g-10</th>\n      <th>g-11</th>\n      <th>...</th>\n      <th>gce_std</th>\n      <th>gce_skew</th>\n      <th>gce_kurt</th>\n      <th>gce_median</th>\n      <th>ge_sum</th>\n      <th>ge_mean</th>\n      <th>ge_std</th>\n      <th>ge_skew</th>\n      <th>ge_kurt</th>\n      <th>ge_median</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.123421</td>\n      <td>-0.433396</td>\n      <td>-0.964591</td>\n      <td>-0.282548</td>\n      <td>-1.016012</td>\n      <td>-0.045026</td>\n      <td>0.698549</td>\n      <td>-0.307208</td>\n      <td>1.545668</td>\n      <td>0.167332</td>\n      <td>...</td>\n      <td>-0.231509</td>\n      <td>-1.382206</td>\n      <td>0.332903</td>\n      <td>0.391092</td>\n      <td>-0.466548</td>\n      <td>-0.466548</td>\n      <td>-0.200071</td>\n      <td>-1.323474</td>\n      <td>0.279847</td>\n      <td>-0.599367</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.118038</td>\n      <td>0.262404</td>\n      <td>0.096686</td>\n      <td>1.204425</td>\n      <td>0.695217</td>\n      <td>0.558623</td>\n      <td>-0.474493</td>\n      <td>0.840109</td>\n      <td>-1.250344</td>\n      <td>-0.585397</td>\n      <td>...</td>\n      <td>-0.311570</td>\n      <td>-0.121494</td>\n      <td>0.771762</td>\n      <td>0.957949</td>\n      <td>-0.069741</td>\n      <td>-0.069741</td>\n      <td>-0.230676</td>\n      <td>-0.019631</td>\n      <td>0.671087</td>\n      <td>0.448958</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.774613</td>\n      <td>1.417783</td>\n      <td>-0.114154</td>\n      <td>-0.020037</td>\n      <td>1.495514</td>\n      <td>0.358201</td>\n      <td>0.044292</td>\n      <td>1.247845</td>\n      <td>-0.655505</td>\n      <td>-0.777878</td>\n      <td>...</td>\n      <td>0.729941</td>\n      <td>0.040962</td>\n      <td>0.375723</td>\n      <td>-0.627145</td>\n      <td>-0.682173</td>\n      <td>-0.682173</td>\n      <td>0.757799</td>\n      <td>-0.153181</td>\n      <td>0.254737</td>\n      <td>-0.724672</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.744187</td>\n      <td>-0.456376</td>\n      <td>0.772458</td>\n      <td>2.448425</td>\n      <td>-0.858771</td>\n      <td>0.298735</td>\n      <td>-0.139522</td>\n      <td>-1.371561</td>\n      <td>-1.013221</td>\n      <td>-0.505054</td>\n      <td>...</td>\n      <td>1.102178</td>\n      <td>-0.094962</td>\n      <td>-1.108978</td>\n      <td>-1.260776</td>\n      <td>-0.239230</td>\n      <td>-0.239230</td>\n      <td>1.017870</td>\n      <td>0.557489</td>\n      <td>-1.011274</td>\n      <td>-0.109471</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.457396</td>\n      <td>0.962230</td>\n      <td>0.981547</td>\n      <td>1.454602</td>\n      <td>-0.869376</td>\n      <td>-0.230595</td>\n      <td>-0.981388</td>\n      <td>0.850767</td>\n      <td>-0.346712</td>\n      <td>-0.711659</td>\n      <td>...</td>\n      <td>0.717398</td>\n      <td>-2.232172</td>\n      <td>1.788667</td>\n      <td>0.710104</td>\n      <td>-0.933620</td>\n      <td>-0.933620</td>\n      <td>0.757162</td>\n      <td>-2.147718</td>\n      <td>1.589138</td>\n      <td>-0.000033</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23808</th>\n      <td>0.236639</td>\n      <td>0.207837</td>\n      <td>-0.350717</td>\n      <td>-0.361947</td>\n      <td>0.575774</td>\n      <td>-0.276129</td>\n      <td>-0.860194</td>\n      <td>-0.022794</td>\n      <td>0.705877</td>\n      <td>-0.706175</td>\n      <td>...</td>\n      <td>-3.097103</td>\n      <td>-0.010938</td>\n      <td>-1.357223</td>\n      <td>0.846992</td>\n      <td>1.399084</td>\n      <td>1.399084</td>\n      <td>-2.973969</td>\n      <td>-0.201149</td>\n      <td>-1.318300</td>\n      <td>1.022804</td>\n    </tr>\n    <tr>\n      <th>23809</th>\n      <td>0.206721</td>\n      <td>-0.253901</td>\n      <td>-0.789493</td>\n      <td>-0.718753</td>\n      <td>0.923262</td>\n      <td>0.508876</td>\n      <td>-0.554977</td>\n      <td>-0.033930</td>\n      <td>0.286984</td>\n      <td>0.192944</td>\n      <td>...</td>\n      <td>-0.112217</td>\n      <td>-2.063024</td>\n      <td>1.858930</td>\n      <td>0.629524</td>\n      <td>0.068775</td>\n      <td>0.068775</td>\n      <td>-0.050632</td>\n      <td>-2.035350</td>\n      <td>1.766637</td>\n      <td>0.374497</td>\n    </tr>\n    <tr>\n      <th>23810</th>\n      <td>-1.940733</td>\n      <td>-0.603128</td>\n      <td>1.308103</td>\n      <td>-1.058194</td>\n      <td>0.856762</td>\n      <td>-0.736422</td>\n      <td>0.651861</td>\n      <td>0.648623</td>\n      <td>-0.303515</td>\n      <td>1.757062</td>\n      <td>...</td>\n      <td>-0.216652</td>\n      <td>-0.060783</td>\n      <td>-0.178504</td>\n      <td>-0.345776</td>\n      <td>0.067837</td>\n      <td>0.067837</td>\n      <td>-0.242659</td>\n      <td>-0.186023</td>\n      <td>-0.149545</td>\n      <td>-0.529686</td>\n    </tr>\n    <tr>\n      <th>23812</th>\n      <td>0.810708</td>\n      <td>0.421287</td>\n      <td>0.315457</td>\n      <td>1.073907</td>\n      <td>-0.021461</td>\n      <td>0.085268</td>\n      <td>0.555475</td>\n      <td>-1.184586</td>\n      <td>0.738031</td>\n      <td>0.152950</td>\n      <td>...</td>\n      <td>1.135048</td>\n      <td>-1.711244</td>\n      <td>-0.674890</td>\n      <td>2.056726</td>\n      <td>-0.815510</td>\n      <td>-0.815510</td>\n      <td>1.176933</td>\n      <td>-1.586650</td>\n      <td>-0.874018</td>\n      <td>1.813982</td>\n    </tr>\n    <tr>\n      <th>23813</th>\n      <td>-1.265053</td>\n      <td>-0.286199</td>\n      <td>1.096373</td>\n      <td>-0.551023</td>\n      <td>-2.097983</td>\n      <td>1.457567</td>\n      <td>2.179353</td>\n      <td>1.263361</td>\n      <td>2.134326</td>\n      <td>1.267512</td>\n      <td>...</td>\n      <td>1.273049</td>\n      <td>-1.709709</td>\n      <td>-0.643381</td>\n      <td>-1.476488</td>\n      <td>-1.289733</td>\n      <td>-1.289733</td>\n      <td>1.112746</td>\n      <td>-1.046423</td>\n      <td>-0.569578</td>\n      <td>-1.361960</td>\n    </tr>\n  </tbody>\n</table>\n<p>21948 rows × 947 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}